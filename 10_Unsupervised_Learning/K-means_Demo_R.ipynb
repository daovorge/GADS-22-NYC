{"nbformat_minor": 0, "nbformat": 4, "cells": [{"execution_count": 1, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import matplotlib as mpl\n", "mpl.style.use('ggplot')\n", "from sklearn.datasets import make_blobs\n", "from sklearn.cluster import KMeans\n", "from sklearn.decomposition import PCA"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#The K-means Algorithm & SKlearn\n", "---\n", "\n", "##Let's start by generating some artificial blobs of data:\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["m = 100\n", "N = 2\n", "num_blobs = 8\n", "\n", "#the y below  contains the 'cluster' information - i.e. which cluster and given data point belongs to\n", "X, y = make_blobs(n_samples = m, n_features = N, centers=num_blobs, cluster_std=0.5, random_state=2)"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": ["#help(make_blobs)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### As usual, we first *plot* the data to get a feeling of what we're dealing with\n", "###Without knowing ahead of time you could argue that there are 3 clusters present"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": ["fig = plt.figure(figsize=(10,5))\n", "ax = plt.subplot(121)\n", "ax.set_title(\"Blobs\")\n", "ax.set_xlabel(\"X1\")\n", "ax.set_ylabel(\"X2\")\n", "ax.scatter(X[:,0], X[:,1], marker = 'o')\n", "\n", "ax1 = plt.subplot(122)\n", "ax1.set_title(\"Actual Blobs\")\n", "ax1.set_xlabel(\"X1\")\n", "ax1.set_ylabel(\"X2\")\n", "ax1.scatter(X[:,0], X[:,1], marker = 'o', c=y)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["##Normally, you would not know about 'y'\n", "##You now use K-means to try and recover this infomation from the data itself"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Check out the docs on sklearn's [K-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": ["#help(KMeans)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["##### class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances=True, verbose=0, random_state=None, copy_x=True, n_jobs=1)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["#Illustrates the basic usage of K-means\n", "#K sets the number of clusters you would like to try to model your data with\n", "K = 2\n", "\n", "#The call to K-means\n", "kmeans1 = KMeans(n_clusters=K, max_iter = 10000, n_init = 100, tol = 1e-6, n_jobs = 1, \\\n", "                 random_state=8, verbose=False)\n", "\n", "#A call to K-means fit returns the cluster number that each data point belongs to\n", "y_hat = kmeans1.fit(X).labels_\n", "\n", "#Intertia is the same thing as cost, this is J, the cost function\n", "inertia = kmeans1.inertia_\n", "\n", "#Centroids holds the coordinates of the cluster centers\n", "centroids = kmeans1.cluster_centers_\n", "\n", "#Check out the results of printing this stuff out\n", "print \"Y hat: \", y_hat\n", "print \"Centroid locations: \\n\", centroids\n", "print \"Inertia: \", inertia\n", "\n", "\n", "#get some new blob data\n", "m_new = 20\n", "N_new = 2\n", "num_blobs_new = 15\n", "\n", "#the y below  contains the 'cluster' information - i.e. which cluster and given data point belongs to\n", "X_new, y_new = make_blobs(n_samples = m_new, n_features = N_new, centers=num_blobs_new, \n", "                          cluster_std=5.0, random_state=2)\n", "\n", "#use the K-means to assign the new data to the 2 centroids\n", "y_new_hat = kmeans1.fit(X_new).labels_\n", "print y_new_hat\n", "\n", "#assign the centroid location to 'centroids'\n", "centroids = kmeans1.cluster_centers_\n", "\n", "fig = plt.figure(figsize = (7, 7))\n", "ax = plt.subplot(111)\n", "ax.scatter(X_new[y_new_hat==1,0], X_new[y_new_hat==1,1], color='blue', marker='o', label='New Data')\n", "ax.scatter(X_new[y_new_hat==0,0], X_new[y_new_hat==0,1], color='green', marker='o', label='New Data')\n", "ax.scatter(centroids[:,0], centroids[:,1], color='red', marker='o', s=100, label='Centroids')\n", "ax.legend(loc='best')\n", "ax.set_title(\"Assigning New Data to Cluster Centers\")\n", "ax.set_xlabel(\"x1\")\n", "ax.set_ylabel(\"x2\")"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Now the label assignments should be quite similar to `Y`, up to a different ordering of the colors:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": ["K = 8\n", "kmeans1 = KMeans(n_clusters=K, max_iter = 10000, n_init = 100, tol = 1e-6, n_jobs = 1, \\\n", "                 random_state=8, verbose=False)\n", "y_hat = kmeans1.fit(X).labels_\n", "inertia = kmeans1.inertia_\n", "centroids = kmeans1.cluster_centers_\n", "\n", "print \"Y hat: \", y_hat\n", "print \"Centroid locations: \\n\", centroids\n", "print \"Inertia: \", inertia\n", "\n", "\n", "fig = plt.figure(figsize=(10,5))\n", "\n", "ax = plt.subplot(121)\n", "ax.set_title(\"Blobs - Original\")\n", "ax.set_xlabel(\"X1\")\n", "ax.set_ylabel(\"X2\")\n", "\n", "#color this plot using the known blob information held in 'y'\n", "ax.scatter(X[:,0], X[:,1], marker = 'o', c=y)\n", "\n", "ax1 = plt.subplot(122)\n", "ax1.set_title(\"Blobs - Recovered by K-means\")\n", "ax1.set_xlabel(\"X1\")\n", "ax1.set_ylabel(\"X2\")\n", "\n", "#color this plot using the predicted cluster information held in 'y_hat'\n", "ax1.scatter(X[:,0], X[:,1], marker = 'o', c=y_hat)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["##Often, you are more interested in the centroids themselves rather than which data points are assigned to a particular centroid or cluster\n", "##The centroids can be seen as *representatives* of their respective cluster"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": ["fig = plt.figure(figsize=(10,5))\n", "\n", "ax = plt.subplot(121)\n", "ax.set_title(\"Blobs & Centroids - Recovered by K-means\")\n", "ax.set_xlabel(\"X1\")\n", "ax.set_ylabel(\"X2\")\n", "ax.scatter(X[:,0], X[:,1], marker = 'o', c=y_hat)\n", "\n", "ax.scatter(centroids[:,0], centroids[:,1], s=100, color = 'black')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## Let's try to find an ideal value of K using the elbow method"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": ["inertia_list = []\n", "k_list = []\n", "for k in xrange(1, 21):\n", "    \n", "    # create a new KMeans object for each value of k\n", "    kmeans = KMeans(n_clusters = k, max_iter = 1000, n_init = 100, tol = 1e-6, n_jobs = -1, \\\n", "                 random_state=8, verbose=False)\n", "    kmeans.fit(X)\n", "    \n", "    print \"k is {:d}, inertia is  {:5.2f}\".format(k, kmeans.inertia_)\n", "    \n", "    # inertia: Sum of distances of samples to their closest cluster center\n", "    inertia_list.append(kmeans.inertia_) \n", "    k_list.append(k)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": ["ideal_k = num_blobs - 1 # (we know in advance that this is the ideal value for k)\n", "\n", "# plot elbow curve\n", "fig = plt.figure(figsize=(8,6))\n", "ax = fig.add_subplot(111)\n", "ax.plot(k_list, inertia_list, 'b*-')\n", "\n", "# highlight the ideal k\n", "ax.plot(k_list[ideal_k], inertia_list[ideal_k], marker='o', markersize=12, \n", "      markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n", "\n", "#plt.grid(True)\n", "ax.set_xlabel('Number of clusters')\n", "ax.set_ylabel('Sum of distances of samples to their closest cluster center')\n", "ax.set_title('\"Elbow\" for K-Means clustering')  "], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["##Now consider the case of 10-dimensional data!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": ["m = 100\n", "#N is TEN!!\n", "N = 10\n", "num_blobs = 8\n", "\n", "\n", "\n", "#MAKE 10 DIMENSIONAL BLOBS\n", "#the y below  contains the 'cluster' information - i.e. which cluster and given data point belongs to\n", "X, y = make_blobs(n_samples = m, n_features = N, centers=num_blobs, cluster_std=2.0, random_state=2)"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["---\n", "##Make sure you really understand 10 dimensional data\n", "###Look at it via a data frame\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": ["cols = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10']\n", "df = pd.DataFrame(X, columns = cols)\n", "df.columns.name = 'N'\n", "df.index.name = 'm'\n", "df.head(5)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": ["df.tail(5)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Search for the ideal value of K\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": ["inertia_list = []\n", "k_list = []\n", "\n", "for k in xrange(1, 31):\n", "    \n", "    # create a new KMeans object for each value of k\n", "    kmeans = KMeans(n_clusters = k, max_iter = 1000, n_init = 100, tol = 1e-6, n_jobs = -1, \\\n", "                 random_state=8, verbose=False)\n", "    kmeans.fit(X)\n", "    \n", "    #print \"k is {:d}, inertia is  {:5.2f}\".format(k, kmeans.inertia_)\n", "    \n", "    # inertia: Sum of distances of samples to their closest cluster center\n", "    inertia_list.append(kmeans.inertia_) \n", "    k_list.append(k)\n", "    \n", "ideal_k = num_blobs - 1 # (we know in advance that this is the ideal value for k)\n", "\n", "# plot elbow curve\n", "fig = plt.figure(figsize=(8,6))\n", "ax = fig.add_subplot(111)\n", "ax.plot(k_list, inertia_list, 'b*-')\n", "\n", "# highlight the ideal k\n", "ax.plot(k_list[ideal_k], inertia_list[ideal_k], marker='o', markersize=12, \n", "      markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n", "\n", "#plt.grid(True)\n", "ax.set_xlabel('Number of clusters')\n", "ax.set_ylabel('Inertia, Cost or J')\n", "ax.set_title('\"Elbow\" for K-Means clustering')  "], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": ["def plot(X1, X2, plot_location, the_title, the_ydim):\n", "    '''A basic plotting routine for plotting a point'''\n", "    \n", "    ax = plt.subplot(plot_location)\n", "    ax.set_title(the_title)\n", "    ax.set_xlabel(\"X0\")\n", "    ax.set_ylabel(the_ydim)\n", "    ax.scatter(X1, X2, marker = 'o', c=y)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "##Visualizing this data\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "##Plot one dimensional vs all the others???!!!\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": ["fig = plt.figure(figsize=(15, 15))\n", "\n", "#Plot dimension 0 against all other 9 dimensions!\n", "for i in xrange(1, N):\n", "    l = '33' + str(i)\n", "    the_title = \"Dimension 0 vs {:s}\".format(str(i))\n", "    the_ydim = \"X{:s}\".format(str(i))\n", "    \n", "    #make the plot\n", "    plot(X[:, 0], X[: ,i], l, the_title, the_ydim)\n", "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2) "], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Be sure that you understand the difference between K-means and PCA\n", "##PCA is dimensional reduction - reducing N, the number of features\n", "##K-means is clustering - potentially reducing m, the number of examples\n", "##K-means has NOTHING to do with altering the dimensionality of your data\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Let's combine the 2, and use PCA to help us visualize the data!!\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "##So how good was that 10-dimensional clustering\n", "##Let's reduce our 10-D data to 2-D and see what it looks like\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": ["#find 8 clusters, let's assume we got a good indication K should be 8\n", "kmeans = KMeans(n_clusters = 8, max_iter = 1000, n_init = 100, tol = 1e-6, n_jobs = -1, \\\n", "                 random_state=8, verbose=False)\n", "kmeans.fit(X)\n", "\n", "#Use the cluster centroids to fit PCA, with 2 components, x1, and x2, because we want to do simple plots\n", "centroids = kmeans.cluster_centers_\n", "myPCA = PCA(n_components = 2).fit(centroids)\n", "\n", "#transform the centroids down to 2D\n", "new_centroids = myPCA.transform(centroids)\n", "\n", "#transform the data down to 2D\n", "new_data = myPCA.transform(X)\n", "\n", "print new_data.shape"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": ["#and now plot in 2-D\n", "\n", "fig = plt.figure(figsize=(7,7))\n", "ax = plt.subplot(111)\n", "ax.scatter(new_centroids[:,0], new_centroids[:,1], marker='o', color='red', s=100, label='Centroids')\n", "ax.scatter(new_data[:,0], new_data[:,1], marker='.', color='green', label='Dimensionally reduced 10D data')\n", "ax.set_title('10 Dimensional Data Reduced to 2-D')\n", "ax.set_xlabel('x1')\n", "ax.set_ylabel('x2')\n", "ax.legend(loc='lower right')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Have I convinced you that these 2 algorithms do work?? K-means can find high-dimensional cluster centers\n", "#PCA can effectively reduce the dimensionality of your data\n", "---"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.9", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}