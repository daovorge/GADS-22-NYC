{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "#This is recommended when using gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Latent Dirichlet Allocation\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The gensim tutorial](https://radimrehurek.com/gensim/tut1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####From Wikipedia, Latent Dirichlet Allocation\n",
    "\n",
    "1. Tell the alogorithm how many topics you think there are\n",
    " - intuitively\n",
    " - statistically\n",
    "\n",
    "2. Assign every word to a topic in a semi-random manner (a dirichlet distribution)\n",
    " - a word can appear in more than one topic\n",
    "\n",
    "3. Iterate: Loop through every word in each topic and update it's topic assignemnt, according to:\n",
    "\n",
    "a. how prevalent is a word across topics, \n",
    "\n",
    "b. how prevalent are topics in the document\n",
    "\n",
    "Looking at each topic what proportion of the topic is down to each word. Certain words will favor certain topics.\n",
    "\n",
    "Looking at each document how prevalent are the topics. Divide up the document into the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I eat fish and vegetables\n",
    "- Fish are pets\n",
    "- My kitten eats fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Ask for 2 topics:\n",
    "\n",
    "Topic A: eat fish, eats fish, vegetables\n",
    "    \n",
    "Topic B: Fish, pets, kitten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Infer the content spread of each sentence by word count\n",
    "\n",
    "- Sentence 1: 100% Topic A\n",
    "\n",
    "- Sentence 2: 100% Topic B\n",
    "\n",
    "- Sentence 3: 33% Topic B and 66% Topic A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Can derive the porportions that each word constitutes in given topics\n",
    "\n",
    "- Topic A might comprise words in the following proportions: 40% eat, 40% fish, 20%vegetables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Documents represented as strings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine computer interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Tokenize the documents, remove stop words and words that only appear once in the corpus\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Firstly let's tokenize the documents, and remove stop words using a 'toy' stop-word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['a', 'and', 'for', 'of', 'to', 'in', 'the'])\n"
     ]
    }
   ],
   "source": [
    "stop_list = set(['for', 'a', 'of', 'the', 'and', 'to', 'in'])\n",
    "print stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents_without_stops = []\n",
    "for docs in documents:\n",
    "    t = [word for word in docs.lower().split() if word not in stop_list]\n",
    "    documents_without_stops.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'computer', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print documents_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in documents_without_stops:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in documents_without_stops:\n",
    "    t = [token for token in text if frequency[token] > 1]\n",
    "    texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'computer', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Now convert documents to vectors, this is a bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary is  Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print \"dictionary is \", dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####There are 12 distinct words, so each document will be represented by a 12-D vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####It is possible also to display the token id's that the words have been mapped to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary to token {u'minors': 11, u'graph': 10, u'system': 5, u'trees': 9, u'eps': 8, u'computer': 0, u'survey': 4, u'user': 7, u'human': 1, u'time': 6, u'interface': 2, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    "print \"Dictionary to token\", (dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The function doc2bow is like the python CountVectorizer. It counts frequency of occurrence of words in each document and returns a spares matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'computer', 'interface', 'computer']\n",
      "[(0, 2), (1, 1), (2, 1)]\n",
      "['survey', 'user', 'computer', 'system', 'response', 'time']\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "['eps', 'user', 'interface', 'system']\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "['system', 'human', 'system', 'eps']\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "['user', 'response', 'time']\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "['trees']\n",
      "[(9, 1)]\n",
      "['graph', 'trees']\n",
      "[(9, 1), (10, 1)]\n",
      "['graph', 'minors', 'trees']\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "['graph', 'minors', 'survey']\n",
      "[(4, 1), (10, 1), (11, 1)]\n",
      "\n",
      "\n",
      "\n",
      "[(0, 2), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print text\n",
    "    print dictionary.doc2bow(text)\n",
    "    \n",
    "print \"\\n\\n\"\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#remember there are 12 tokens, and you need the dictionary to token information to work out the coding\n",
    "\n",
    "for c in corpus:\n",
    "    print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The LDA model converts the bag-of-words representation into a topic-space of lower dimensionality\n",
    "#####LDA's topics are probability distributions over words\n",
    "#####The distributions are inferred automatically from the corpus\n",
    "#####Documents are then interpretted as a mixture of these topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2, passes = 10, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topic 0:\n",
      "0.173*graph + 0.172*trees\n",
      "\n",
      "\n",
      "topic 1:\n",
      "0.161*computer + 0.159*user\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda_model.print_topics(num_topics = 2, num_words = 2)):\n",
    "    print \"\\n\\ntopic {:d}:\\n\".format(i), topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Classification to topic, with accompanying probability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_doc = 'the grass is greener'\n",
    "new_doc1 = 'Human Computer Interaction'\n",
    "new_doc2 = 'Graphs are excellent data structures and are related to trees'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec1 = dictionary.doc2bow(new_doc1.lower().split())\n",
    "new_vec2 = dictionary.doc2bow(new_doc2.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Divides the documents up into topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5), (1, 0.5)]\n",
      "[(0, 0.20654388799196441), (1, 0.79345611200803567)]\n",
      "[(0, 0.74678800888043906), (1, 0.25321199111956094)]\n"
     ]
    }
   ],
   "source": [
    "print lda_model[new_vec]\n",
    "print lda_model[new_vec1]\n",
    "print lda_model[new_vec2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaModel in module gensim.models.ldamodel object:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC)\n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=0.01)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `eps`).\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a matrix of shape num_topics x num_words,\n",
      " |      which can be used to impose asymmetric priors over the word\n",
      " |      distribution on a per-topic basis. This may be useful if you\n",
      " |      want to seed certain topics with particular words by boosting\n",
      " |      the priors for those words.\n",
      " |      \n",
      " |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      " |      on how to set up a cluster of machines for gensim).\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      " |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      " |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      " |      document (=2d matrix=what comes out of `inference()`).\n",
      " |      If not supplied, will be inferred from the model.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  print_topic(self, topicid, topn=10)\n",
      " |      Return the result of `show_topic`, but formatted as a single string.\n",
      " |  \n",
      " |  print_topics(self, num_topics=10, num_words=10)\n",
      " |  \n",
      " |  save(self, fname, *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Return a list of `(words_probability, word)` 2-tuples for the most probable\n",
      " |      words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      For `num_topics` number of topics, return `num_words` most significant words\n",
      " |      (10 words per topic, by default).\n",
      " |      \n",
      " |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      " |      True, or a list of (probability, word) 2-tuples if False.\n",
      " |      \n",
      " |      If `log` is True, also output this result to log.\n",
      " |      \n",
      " |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      " |      arbitrary and may change between two LDA training runs.\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      " |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      " |      Table 1 in Hoffman et al.)\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |      \n",
      " |      Uses Newton's method, described in **Huang: Maximum Likelihood Estimation of Dirichlet Distribution Parameters.** (http://www.stanford.edu/~jhuang11/research/dirichlet/dirichlet.pdf)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(cls, fname, *args, **kwargs) from __builtin__.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays are mmap'ed back as read-only (shared memory).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
