{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import operator\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#The Iris Dataset Part 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#The Iris Dataset is a builtin dataset, you need from sklearn import datasets - see above\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Construct a Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {'class':iris.target, 'sepal_length':iris.data[:,0], 'sepal_width':iris.data[:,1], \\\n",
    "      'petal_length':iris.data[:,2], 'petal_width':iris.data[:,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'petal_length', u'petal_width', u'sepal_length', u'sepal_width'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#We will need a list of features only\n",
    "features = df.columns[1:]\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.7</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.0</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.0</td>\n",
       "      <td> 6.5</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.4</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.2</td>\n",
       "      <td> 3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 5.9</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  petal_length  petal_width  sepal_length  sepal_width\n",
       "145      2           5.2          2.3           6.7          3.0\n",
       "146      2           5.0          1.9           6.3          2.5\n",
       "147      2           5.2          2.0           6.5          3.0\n",
       "148      2           5.4          2.3           6.2          3.4\n",
       "149      2           5.1          1.8           5.9          3.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Put the scaled data into a new dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True).fit(df[features])\n",
    "df_scaled = pd.DataFrame(scaler.transform(df[features]), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scaled['class'] = df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-0.900681</td>\n",
       "      <td> 1.032057</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.385353</td>\n",
       "      <td> 0.337848</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.506521</td>\n",
       "      <td> 0.106445</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.021849</td>\n",
       "      <td> 1.263460</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width  sepal_length  sepal_width  class\n",
       "0     -1.341272    -1.312977     -0.900681     1.032057      0\n",
       "1     -1.341272    -1.312977     -1.143017    -0.124958      0\n",
       "2     -1.398138    -1.312977     -1.385353     0.337848      0\n",
       "3     -1.284407    -1.312977     -1.506521     0.106445      0\n",
       "4     -1.341272    -1.312977     -1.021849     1.263460      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Define a custom confusion matrix function \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix(predictions, y, names):\n",
    "    '''This function uses the pd.crosstab function to create a confusion matrix:\n",
    "    predictions are the predictions from the predictive mode\n",
    "    y are the known class labels\n",
    "    names are the names of the features used in the model'''\n",
    "    \n",
    "    cf = pd.crosstab(y, predictions)\n",
    "    cf.columns = names\n",
    "    cf.index = names\n",
    "    cf.columns.name = 'Prediction'\n",
    "    cf.index.name = 'Actual'\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Full Training with Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_optimal_features(dfs, target, all_features, \n",
    "                          max_input_dimension = 5,\n",
    "                          degree_list=[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10],\n",
    "                          reg_list = [1.0]):\n",
    "    \n",
    "    #initialize some results dictionarys\n",
    "    results = {}\n",
    "    results_vars  = {}\n",
    "    temp_models = {}\n",
    "    models = {}\n",
    "    score = {}\n",
    "    #count will be used to index the dictionarys\n",
    "    count = 0\n",
    "    \n",
    "    #Convert the dataframe data to user-friendly arrays\n",
    "    X = dfs[all_features].values\n",
    "    y = dfs[target].values\n",
    "   \n",
    "    #for classification I use the Stratified Shuffle Split for cross validation purposes\n",
    "    sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=32) \n",
    "    \n",
    "    #split the data first and the training set is what is used for all model parameters!!\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "        \n",
    "        #look for a good degree for the polynomial features\n",
    "        for degree in degree_list:\n",
    "            the_in_dim = max_input_dimension\n",
    "            \n",
    "            pf_model = PolynomialFeatures(degree).fit(X_train)\n",
    "            pf = pf_model.transform(X_train)\n",
    "        \n",
    "            #if the polynomial features are not enough to satisfy the max input dimension, as specified by the user\n",
    "            #then reduce the max input dimension\n",
    "            if the_in_dim > pf.shape[1]:\n",
    "                the_in_dim = pf.shape[1]\n",
    "            \n",
    "            #now use PCA to try models of varying dimensions\n",
    "            for input_dimension in xrange(1, the_in_dim + 1):\n",
    "        \n",
    "                myPCA = PCA(n_components = input_dimension).fit(pf)\n",
    "            \n",
    "                #The final transformed dataset\n",
    "                X_transform = myPCA.transform(pf)\n",
    "        \n",
    "                #Regularize the model\n",
    "                for reg_C in reg_list:\n",
    "                \n",
    "                    #Finally build and fit the logistic regression model\n",
    "                    clfLR = LogisticRegression(C=reg_C)\n",
    "                    clfLR.fit(X_transform, y_train)\n",
    "    \n",
    "                    #Now test the performance on the validation set\n",
    "                    #But first we have to prepare the test set data\n",
    "                    #So get the non-linear features using the polynomial model\n",
    "                    pf_test = pf_model.transform(X_test)\n",
    "                    \n",
    "                    #Now get the PCA features using the PCA model\n",
    "                    X_test_transform = myPCA.transform(pf_test)\n",
    "                    \n",
    "                    #And finally get a predication from the logistic regression model\n",
    "                    my_score = clfLR.score(X_test_transform, y_test)\n",
    "                    \n",
    "                    #We accumulate all results as we might like to look at more than just the 'best'\n",
    "                    m={}\n",
    "                    m['score'] = my_score\n",
    "                    m['indim'] = input_dimension\n",
    "                    m['deg'] = degree\n",
    "                    m['reg'] = reg_C\n",
    "                    m['clf'] = clfLR\n",
    "                    m['xtrain'] = X_transform\n",
    "                    m['ytrain'] = y_train\n",
    "                    m['xtest'] = X_test_transform\n",
    "                    m['ytest'] = y_test\n",
    "                    m['poly'] = pf_model\n",
    "                    m['pca'] = myPCA\n",
    "                    \n",
    "                    #print \"here\",  m['ascore']\n",
    "                    temp_models[count] = m\n",
    "                    score[count] = my_score\n",
    "                    count += 1\n",
    "                    \n",
    "    #sort the score dictionary\n",
    "    score = sorted(score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #and then using the sorted score dictionary sort the main results dictionary\n",
    "    for i in xrange(len(score)):\n",
    "        models[i] = temp_models[score[i][0]]\n",
    "        \n",
    "    #now just return the sorted resulst dictionary\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up the call\n",
    "models = find_optimal_features(df_scaled, 'class', features, \n",
    "                                           max_input_dimension = 10, \n",
    "                                           degree_list = [2, 3, 4, 5, 6, 7], \n",
    "                                           reg_list = np.logspace(-4, 6, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986666666667\n",
      "LogisticRegression(C=2.7825594022071258, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
      "          random_state=None, tol=0.0001)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Each model is access by an index into models, but they are sorted\n",
    "#The model with the best score is always model[0], and the next best is model[1], etc etc etc...\n",
    "#Then each results field can be access by name\n",
    "#As you will see in the forth coming cells this makes things simpler and more intelligible\n",
    "print models[0]['score']\n",
    "print models[0]['clf']\n",
    "print models[0]['indim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 0\n",
      "score =  0.98667 In Dim =   7 Deg =  3 C =  2.7826\n",
      "\n",
      "\n",
      "model number 1\n",
      "score =  0.97333 In Dim =   6 Deg =  2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 2\n",
      "score =  0.97333 In Dim =   7 Deg =  2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 3\n",
      "score =  0.97333 In Dim =   8 Deg =  2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 4\n",
      "score =  0.97333 In Dim =   9 Deg =  2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 5\n",
      "score =  0.97333 In Dim =   9 Deg =  2 C =  2.7826\n",
      "\n",
      "\n",
      "model number 6\n",
      "score =  0.97333 In Dim =  10 Deg =  2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 7\n",
      "score =  0.97333 In Dim =  10 Deg =  2 C =  2.7826\n",
      "\n",
      "\n",
      "model number 8\n",
      "score =  0.97333 In Dim =   5 Deg =  3 C = 35.9381\n",
      "\n",
      "\n",
      "model number 9\n",
      "score =  0.97333 In Dim =   5 Deg =  3 C = 464.1589\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's examine the top ten models, and check out the input dimension, the polynomial degree and the regularizer\n",
    "for i in xrange(10):\n",
    "    print \"model number {:1d}\".format(i)\n",
    "    print \"score = {:8.5}\".format(models[i]['score']), \\\n",
    "    \"In Dim = {:3d}\".format(models[i]['indim']), \\\n",
    "    \"Deg = {:2d}\".format(models[i]['deg']), \\\n",
    "    \"C = {:7.4f}\".format(models[i]['reg'])\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model score = 0.98667\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           1         24\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#But we are not done yet. We need to look at the confusion matrices for all the top models\n",
    "\n",
    "for i in xrange(5):\n",
    "    print \"----------------------------------------\"\n",
    "    print \"Model score = {:5.5f}\".format(models[i]['clf'].score(models[i]['xtest'], models[i]['ytest']))\n",
    "    cm = my_confusion_matrix(models[i]['clf'].predict(models[i]['xtest']), models[i]['ytest'], iris.target_names)\n",
    "    print cm\n",
    "    print \"----------------------------------------\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is predicted to belong to class 0 setosa\n",
      "...with the following probabilities:\n",
      "\n",
      "Probability of belonging to the class setosa      =  0.41\n",
      "Probability of belonging to the class versicolor  =  0.29\n",
      "Probability of belonging to the class virginica   =  0.30\n"
     ]
    }
   ],
   "source": [
    "#Using the best model let's predict something new\n",
    "new_input = [1.5, 0.45, 8.2, 3.7]\n",
    "\n",
    "#input scaling first\n",
    "new_input_scaled = scaler.transform(new_input)\n",
    "\n",
    "#now the polynomial features\n",
    "p_input = models[0]['poly'].transform(new_input_scaled)\n",
    "\n",
    "#and now the PCA\n",
    "new_X_transform = models[0]['pca'].transform(p_input)\n",
    "\n",
    "#and now predict using the LR model\n",
    "pred = models[0]['clf'].predict(new_X_transform)\n",
    "\n",
    "\n",
    "print \"This data is predicted to belong to class {:d}\".format(pred[0]), \\\n",
    "    iris.target_names[pred[0]]\n",
    "    \n",
    "prob = models[0]['clf'].predict_proba(new_X_transform)\n",
    "print \"...with the following probabilities:\\n\"\n",
    "for i in xrange(3):\n",
    "    print \"Probability of belonging to the class {:11s} = {:5.2f}\".format(iris.target_names[i],prob.ravel()[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
