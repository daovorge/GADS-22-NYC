{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import operator\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#The Iris Dataset Part 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#The Iris Dataset is a builtin dataset, you need from sklearn import datasets - see above\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Construct a Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {'class':iris.target, 'sepal_length':iris.data[:,0], 'sepal_width':iris.data[:,1], \\\n",
    "      'petal_length':iris.data[:,2], 'petal_width':iris.data[:,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'petal_length', u'petal_width', u'sepal_length', u'sepal_width'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#We will need a list of features only\n",
    "features = df.columns[1:]\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.7</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.0</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.0</td>\n",
       "      <td> 6.5</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.4</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.2</td>\n",
       "      <td> 3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 5.9</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  petal_length  petal_width  sepal_length  sepal_width\n",
       "145      2           5.2          2.3           6.7          3.0\n",
       "146      2           5.0          1.9           6.3          2.5\n",
       "147      2           5.2          2.0           6.5          3.0\n",
       "148      2           5.4          2.3           6.2          3.4\n",
       "149      2           5.1          1.8           5.9          3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scale the data, note I also scaled y\n",
    "## Put the scaled data into a new dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: peform feature scaling on the original dataframe - 0 mean, unit std deviation. \n",
    "#Create a new dataframe for the scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-0.900681</td>\n",
       "      <td> 1.032057</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.385353</td>\n",
       "      <td> 0.337848</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.506521</td>\n",
       "      <td> 0.106445</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.021849</td>\n",
       "      <td> 1.263460</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width  sepal_length  sepal_width  class\n",
       "0     -1.341272    -1.312977     -0.900681     1.032057      0\n",
       "1     -1.341272    -1.312977     -1.143017    -0.124958      0\n",
       "2     -1.398138    -1.312977     -1.385353     0.337848      0\n",
       "3     -1.284407    -1.312977     -1.506521     0.106445      0\n",
       "4     -1.341272    -1.312977     -1.021849     1.263460      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Define a custom confusion matrix function \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix(predictions, y, names):\n",
    "    '''This function uses the pd.crosstab function to create a confusion matrix:\n",
    "    predictions are the predictions from the predictive mode\n",
    "    y are the known class labels\n",
    "    names are the names of the features used in the model'''\n",
    "    \n",
    "    cf = pd.crosstab(y, predictions)\n",
    "    cf.columns = names\n",
    "    cf.index = names\n",
    "    cf.columns.name = 'Prediction'\n",
    "    cf.index.name = 'Actual'\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Full Training with Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Using the function outline below complete its implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_optimal_features(dfs, target, all_features, \n",
    "                          max_input_dimension = 5,\n",
    "                          degree_list=[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10],\n",
    "                          reg_list = [1.0]):\n",
    "    \n",
    "    #initialize some results dictionarys\n",
    "    results = {}\n",
    "    results_vars  = {}\n",
    "    #count will be used to index the dictionarys\n",
    "    count = 0\n",
    "    \n",
    "    #Convert the dataframe data to user-friendly arrays\n",
    "    #TODO complete the 2 assignment statements below - hint: look at the arguments in the function declaration\n",
    "    #X = \n",
    "    #y = \n",
    "   \n",
    "    #for classification I use the Stratified Shuffle Split for cross validation purposes\n",
    "    #TODO have a quick read about Stratified Shuffle Split in the Sklearn documentation\n",
    "    #sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=32) \n",
    "    \n",
    "    #split the data first and the training set is what is used for all model parameters!!\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "        \n",
    "        #look for a good degree for the polynomial features\n",
    "        for degree in degree_list:\n",
    "            \n",
    "            #TODO: Add 2 lines of code to 1. fit polynomial features, and 2. transform the training data\n",
    "            #pf_model = \n",
    "            #pf = \n",
    "        \n",
    "            #if the polynomial features are not enough to satisfy the max input dimension, as specified by the user\n",
    "            #then reduce the max input dimension\n",
    "            if max_input_dimension > pf.shape[1]:\n",
    "                max_input_dimension = pf.shape[1]\n",
    "            \n",
    "            #now use PCA to try models of varying dimensions\n",
    "            for input_dimension in xrange(1, max_input_dimension+1):\n",
    "        \n",
    "                #TODO: Add 2 lines of code to 1. fit PCA with the number of components = input_dimension\n",
    "                #You'll be using the polynomial data from above, captured in the variable pf\n",
    "                #myPCA = \n",
    "                #X_transform = \n",
    "        \n",
    "                #Regularize the model\n",
    "                for reg_C in reg_list:\n",
    "                \n",
    "                    #Finally build and fit the logistic regression model\n",
    "                    #TODO: complete 2 lines of code to 1. build a logistic regression model, and 2. fit the model to\n",
    "                    #the transform input data and the class labels. Regularize using reg_C\n",
    "                    #clfLR = \n",
    "                    #\n",
    "    \n",
    "                    #Now test the performance on the validation set\n",
    "                    #But first we have to prepare the test set data\n",
    "                    #So get the non-linear features using the polynomial model\n",
    "                    pf_test = pf_model.transform(X_test)\n",
    "                    \n",
    "                    #Now get the PCA features using the PCA model\n",
    "                    X_test_transform = myPCA.transform(pf_test)\n",
    "                    \n",
    "                    #And finally get a predication from the logistic regression model\n",
    "                    my_score = clfLR.score(X_test_transform, y_test)\n",
    "                    \n",
    "                    #We accumulate all results as we might like to look at more than just the 'best'\n",
    "                    results[count] = my_score\n",
    "                    results_vars[count] = [input_dimension, degree, reg_C, \\\n",
    "                                           clfLR, X_transform, y_train, X_test_transform, y_test, \n",
    "                                           pf_model, myPCA]\n",
    "                    count += 1\n",
    "                    \n",
    "    return (results, results_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up the call to the function\n",
    "#TODO: When you are ready uncomment the lines below to call the function\n",
    "#TODO: Explore different setting for the max_input_dimension, the degree of the polynomial and the regularization list\n",
    "\n",
    "#scores, input_vars = find_optimal_features(df_scaled, 'class', features, \n",
    "                                           #max_input_dimension = 8, \n",
    "                                           #degree_list = [2, 3, 4, 5, 6], \n",
    "                                           #reg_list = np.logspace(-4, 6, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-04,   1.29154967e-03,   1.66810054e-02,\n",
       "         2.15443469e-01,   2.78255940e+00,   3.59381366e+01,\n",
       "         4.64158883e+02,   5.99484250e+03,   7.74263683e+04,\n",
       "         1.00000000e+06])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logspace test area - to see what the regularizer looks like\n",
    "np.logspace(-4, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the top ten models, based upon the scores\n",
    "#TODO: Uncomment the following line of code to get the top ten models sorted in decreasing order of classification scores\n",
    "\n",
    "#top_ten = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's examine the top ten models, and check out the input dimension, the polynomial degree and the regularizer\n",
    "#TODO: Uncomment the following code to check out the parameters involved in the top ten models\n",
    "\n",
    "\n",
    "#for i in xrange(len(top_ten)):\n",
    "#    print \"model number {:d}\".format(top_ten[i][0])\n",
    "#    print \"score = {:5.5}\".format(top_ten[i][1]), \\\n",
    "#    \"In Dim = {:d}\".format(input_vars[top_ten[i][0]][0]), \\\n",
    "#    \"Deg = {:d}\".format(input_vars[top_ten[i][0]][1]), \\\n",
    "#    \"C = {:7.4f}\".format(input_vars[top_ten[i][0]][2])\n",
    "#    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#But we are not done yet. We need to look at the confusion matrices for all the top models\n",
    "#TODO: Uncomment the following code to check out the top ten models' confusion matrices\n",
    "\n",
    "#for i in xrange(5):\n",
    "#    tn = top_ten[i][0]\n",
    "#    model = input_vars[tn][3]\n",
    "#    X_test = input_vars[tn][6]\n",
    "#    y_test = input_vars[tn][7]\n",
    "#    y_hat = model.predict(X_test)\n",
    "\n",
    "#    print \"----------------------------------------\"\n",
    "#    print \"Model score = {:5.5f}\".format(model.score(X_test, y_test))\n",
    "#    cm = my_confusion_matrix(y_hat, y_test, iris.target_names)\n",
    "#    print cm\n",
    "#    print \"----------------------------------------\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Uncomment the following cell to get a prediction for some data from an unknown class of Iris\n",
    "\n",
    "#Here's some data from a new iris - input this data into your model and classify it\n",
    "#new_input = [1.5, 0.45, 8.2, 3.7]\n",
    "\n",
    "#Using the best model let's predict something new\n",
    "#tn = top_ten[0][0]\n",
    "\n",
    "#input scaling first\n",
    "#new_input_scaled = scaler.transform(new_input)\n",
    "\n",
    "#now the polynomial features\n",
    "#pf = input_vars[tn][8]\n",
    "#p_input = pf.transform(new_input_scaled)\n",
    "\n",
    "#and now the PCA\n",
    "#myPCA = input_vars[tn][9]\n",
    "#new_X_transform = myPCA.transform(p_input)\n",
    "\n",
    "#and now predict using the LR model\n",
    "#clfLR = input_vars[tn][3]\n",
    "#pred = clfLR.predict(new_X_transform)\n",
    "#print \"This data is predicted to belong to class {:d}\".format(pred[0]), iris.target_names[pred[0]]\n",
    "#prob = clfLR.predict_proba(new_X_transform)\n",
    "#print \"...with the following probabilities\",\n",
    "#for i in xrange(3):\n",
    "#    print prob.ravel()[i],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
