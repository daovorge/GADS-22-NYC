{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adapted from Probabilistic-Programming-and-Bayesian-Methods-for-Hackers, Chapter 2\n",
    "\n",
    "###From http://www.edwardtufte.com/tufte/ebooks, in \"Visual and Statistical Thinking: Displays of Evidence for Making Decisions\":\n",
    "\n",
    "####On January 28, 1986, the space shuttle Challenger exploded and seven astronauts died because two rubber O-rings leaked. These rings had lost their resiliency because the shuttle was launched on a very cold day. Ambient temperatures were in the low 30s and the O-rings themselves were much colder, less than 20F.\n",
    "\n",
    "####One day before the flight, the predicted temperature for the launch was 26F to 29F. Concerned that the rings would not seal at such a cold temperature, the engineers who designed the rocket opposed launching Challenger the next day.\n",
    "\n",
    "####But they did not make their case persuasively, and were over-ruled by NASA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"/Users/mrgholt/GADS-22-NYC/Images/challenger4.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The image above shows the leak, where the O-ring failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Challenger O-ring Failure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/mrgholt/GADS-22-NYC/Datasets/challenger_oring.csv\", \n",
    "                   header=False,\n",
    "                   names=[\"date\", \"temp\", \"failure\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a new dataframe without the last row\n",
    "data1 = pd.DataFrame(data[data.failure != 'Challenger Accident'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: determine the types of data you have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: convert the failure column to integers, and convert the date column to a consistent date format\n",
    "#using the pd.to_datetime function. Hint: remember the apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You should now have a pandas timestamp type, and 2 integers types for the 3 columns of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Notes:\n",
    "\n",
    "A value of \"1\" represents failure, of the O-ring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Plot the data - Temperature on the x-axis and Failure on the y-axis. Just plot points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Using a simple Linear Regression model (LinearRegression()) fit a line to the data and replot the points \n",
    "#and the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Notes\n",
    "\n",
    "This plot below has a classic sigmoid shape, so one might expect logistic regression to work. Furthermore, we do want to find the probability of failure and make predictions from there.\n",
    "\n",
    "Logistic regression is carried out in the same way as linear. However, there is the matter of setting the regularization co-efficient \"C\"\n",
    "\n",
    "The default C in sklearn is 1. The meaning of C is: the larger the C, the lesser the regularization. The smaller the C the higher the regularization. C must be a positive float. So a C = 1.0, the default, means a high degree of regularization, and a C = 10000, would mean much less regularization.\n",
    "\n",
    "Scikit-learn Logistic Regression can use both L1, and L2 regularization.\n",
    "\n",
    "Remember here, though, that we have just two co-efficents: an intercept, and the outside temperature. So we do not expect to need much regularization. Set C=1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fit a logistic regression model with a C value of 1000. \n",
    "#TODO: Make sure you can find the probabilities and predictions within the model structure, so print out the\n",
    "#probabilities using the x_plot evenly spaced x-axis points defined above. Assign the probabilities to a variable\n",
    "#called 'probabilities'\n",
    "#TODO: Print out the class predictions as well (using x_plot)\n",
    "#Assign the class predictions to a variable called 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Uncomment the plot below and make sure you can see the 3 subsets of data\n",
    "#You will have to make sure your variables replace the ones currently in the cell!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (15,5))\n",
    "#ax = plt.subplot(111)\n",
    "#ax.set_ylim(-0.8, 1.2)\n",
    "\n",
    "#ax.scatter(data1.temp, data1.failure, s=40, color='aqua', marker='o', label='Training Temperature Responses')\n",
    "\n",
    "#ax.plot(x_plot, probabilities, marker='s', color='green', label='Test Temperature Probabilities')\n",
    "\n",
    "#ax.scatter(x_plot, predictions, marker='s', color='black', s=30, alpha=0.7, label='Test Temperature Classifications')\n",
    "\n",
    "#ax.set_title(\"Challenger Disaster Temp vs Failure\")\n",
    "#ax.set_xlabel(\"Temperature (degrees F)\")\n",
    "#ax.set_ylabel(\"Failure (0=No, 1=Yes)\")\n",
    "#ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The logistic regression models also report a 'score'\n",
    "#TODO: modify the code below to print out the score from your logistic regression model\n",
    "#TODO: print out the confusion matrix for your logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 20.0/23.0\n",
    "#print clfLR.score(data1[['temp']], data1['failure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression with K-folds Cross Validation\n",
    "\n",
    "Because the dataset is very small (23 records) we will use K-folds cross validation. Use K=5\n",
    "\n",
    "Using K-folds cross validation let's optimize the value for C\n",
    "\n",
    "Insist that the Logistic Regression model uses L2 by setting the 'penalty' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: In a separate cell test out what np.logspace does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#m = len(data1['temp'].values)\n",
    "\n",
    "#create a list of C values from which to optimize\n",
    "#C_list = list(np.logspace(-4, 5, 100))\n",
    "\n",
    "#Pull the data from pandas suitable for use with K-folds\n",
    "#X = data1['temp'].values\n",
    "#X = X.reshape(m, 1)\n",
    "\n",
    "#y = data1['failure'].values\n",
    "\n",
    "#C increases (not decreases like the linear regression regularization!)\n",
    "#max_score = 1e-99\n",
    "\n",
    "#Traverse over the values in the C_list\n",
    "#for myC in C_list:\n",
    "    \n",
    "    #create the KFolds object\n",
    "#    kf = KFold(m, n_folds = 5)\n",
    "    \n",
    "    #create a list to keep the scores from each model\n",
    "#    score_list = []   \n",
    "    \n",
    "    #Loop over the indices created by the Kfolds object\n",
    "#    for train_index, test_index in kf:\n",
    "        \n",
    "        #create the train and test sets\n",
    "#        X_train, y_train, X_test, y_test = X[train_index], y[train_index], \\\n",
    "#                                            X[test_index], y[test_index]\n",
    "            \n",
    "        #create and fit a model using the value of C and an l2 penalty\n",
    "#        clfLR3 = LogisticRegression(C=myC, penalty='l2')\n",
    "#        clfLR3.fit(X_train, y_train)\n",
    "        \n",
    "        #Extract the score for the model on the validation set and append it to the score list\n",
    "#        score_list.append(clfLR3.score(X_test, y_test))\n",
    "    \n",
    "    #convert the score list to an np array and then take the mean\n",
    "#    sl = np.array(score_list)\n",
    "    \n",
    "    #if the mean of the scores exceeds the maximum score then remember the score and the value of C\n",
    "#    if sl.mean() > max_score:\n",
    "#        max_score = sl.mean()\n",
    "#        max_C_value = myC\n",
    "        \n",
    "    \n",
    "#print out the best score and the maximum value of C    \n",
    "#print max_score, max_C_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Using the value of C from above rebuild and refit the logistic regression model and print out a confusion\n",
    "#matrix for the entire dataset. How does it compare with the original confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Print out the probabilities and the class predictions using x_plot and this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's compare the 2 models\n",
    "#NOTICE the single point for which the classification has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (15,5))\n",
    "#ax = plt.subplot(111)\n",
    "#ax.set_ylim(-0.8, 1.2)\n",
    "\n",
    "#ax.plot(x_plot, probabilities, marker='s', color='magenta', label='Test Temperature Probabilities No Xval')\n",
    "#ax.scatter(data1['temp'], clfLR.predict(data1[['temp']]), marker='s', \\\n",
    "#           color='magenta', s=30, alpha=1.0, label='Test Temperature Classifications No Xval')\n",
    "\n",
    "#ax.plot(x_plot, probabilities4, marker='s', color='black', label='Test Temperature Probabilities With Xval')\n",
    "#ax.scatter(data1['temp'], clfLR4.predict(data1[['temp']]), marker='s', \\\n",
    "#           color='black', s=30, alpha=0.5, label='Test Temperature Classifications with Xval')\n",
    "\n",
    "#ax.set_title(\"Challenger Disaster Temp vs Failure\")\n",
    "#ax.set_xlabel(\"Temperature (degrees F)\")\n",
    "#ax.set_ylabel(\"Failure (0=No, 1=Yes)\")\n",
    "#ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the true story was even worse than our data made it out to be! We did not take the severity of the incidents into account. How could we have incorporated this severity into our analysis? (the following images images are taken from Tufte's booklet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"/Users/mrgholt/GADS-22-NYC/Images/challenger_all_table.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"/Users/mrgholt/GADS-22-NYC/Images/chall-damage.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
