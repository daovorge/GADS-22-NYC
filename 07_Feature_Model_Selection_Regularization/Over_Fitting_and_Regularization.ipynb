{"nbformat_minor": 0, "nbformat": 4, "cells": [{"execution_count": 1, "cell_type": "code", "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import matplotlib as mpl\n", "from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.linear_model import SGDRegressor\n", "from sklearn.linear_model import Ridge\n", "from sklearn.linear_model import Lasso\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.metrics import mean_squared_error\n", "from IPython import display"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 2, "cell_type": "code", "source": ["#Our standard sinusoid generator function\n", "def f(x):\n", "    return np.sin(2.0 * np.pi * x)"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": ["#Get m = 50 points\n", "np.random.seed(9)\n", "m = 50\n", "\n", "#x_plot is evenly spaced points to plot the generator function\n", "x_plot = np.linspace(0, 1, m)\n", "\n", "#unevenly spaced points to generate the training data\n", "X = np.random.uniform(0, 1, size=m)[:, np.newaxis] \n", "y = f(X) + np.random.normal(scale=0.3, size=m)[:, np.newaxis]"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["#The standard plot of the generator function and the training points\n", "\n", "mpl.style.use('ggplot')\n", "fig = plt.figure(figsize=(10,5))\n", "ax = plt.subplot(111)\n", "ax.plot(x_plot, f(x_plot), 'g')\n", "ax.set_xlim(0,1)\n", "ax.set_ylim(-2,2)\n", "ax.set_xlabel('X')\n", "ax.set_ylabel('y')\n", "ax.set_title('Training points')\n", "ax.scatter(X, y)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": ["#Two useful plotting functions, plot_approximation will plot the model predictions, and plot_model_parameters will\n", "#plot the values of the vector theta. \n", "\n", "def plot_approximation(xplot, X, y, clf, ax, llabel=None, show_test=0):\n", "    '''Plot the approximation of clf on axis ax'''\n", "    \n", "    #plot the generator function\n", "    ax.plot(x_plot, f(x_plot), 'g')\n", "    \n", "    #plot the training data\n", "    ax.scatter(X, y, color='b')\n", "    \n", "    #plot the predictions from the model clf\n", "    ax.plot(x_plot, clf.predict(x_plot[:, np.newaxis]), 'r', label=llabel)\n", "    \n", "    ax.set_ylabel('y')\n", "    ax.set_xlabel('X')\n", "    ax.set_ylim((-2, 2))\n", "    ax.set_xlim((0, 1))\n", "    ax.legend(loc='upper right')\n", "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n", "    \n", "def plot_model_parameters(clf, ax, llabel=None, ylim_min=1e-1):\n", "    \"Plot the model parameters of clf on axis ax\"\n", "    \n", "    #This prizes the theta parameters out of make pipeline!\n", "    coef = clf.steps[-1][1].coef_.ravel()\n", "    \n", "    ax.set_yscale('Log')\n", "    ax.set_ylim((ylim_min, 1e8))\n", "    ax.set_ylabel('Absolute Value Model Parameter (Log))')\n", "    ax.set_xlabel('Model Parameter')\n", "    ax.set_xlim((1, 10))\n", "    \n", "    #plot the absolute values of the model parameters\n", "    ax.plot(np.abs(coef), marker='o', label=llabel)\n", "    \n", "    #Also print out the model parameters\n", "    print \"Model Parameters:\"\n", "    mp_list = clf.steps[-1][1].coef_.ravel()\n", "    for i in xrange(1, len(mp_list)):\n", "        print \"{:7.2f}\".format(mp_list[i]),\n", "    print \"\\n\""], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#The problem of over-fitting\n", "---\n", "\n", "##If there are too many features then the hypothesis function may fit the training set extremely well, but fail to 'generalize' well to new examples\n", "##Size of the model - meaning the number and complexity of the features contribute to the model fit\n", "\n", "---\n", "#Model Fit:\n", "---\n", "## 1. Under-fit. The model is too simple and cannot fit the training data. Poor generalization. Called \"High Bias\"\n", "## 2. Over-fit. The model is overly complex and fits the training data extremely well. Poor generalization. \"High Variance\""], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Let's illustrate Bias and Variance\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["fig = plt.figure(figsize=(20,10))\n", "ax1 = plt.subplot(221)\n", "ax2 = plt.subplot(222)\n", "ax3 = plt.subplot(223)\n", "ax4 = plt.subplot(224)\n", "axes = [ax1, ax2, ax3, ax4]\n", "\n", "clf_list = []\n", "\n", "#Build and fit model of increasing degree polynomials, using simple Least Squares Regression\n", "for axis, degree in enumerate([1, 3, 9, 15]):\n", "    clf = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n", "    clf_list.append(clf)\n", "    clf.fit(X, y)\n", "    \n", "    #Plot the generator function, the training data, and the model predictions\n", "    plot_approximation(x_plot, X, y, clf, axes[axis],\\\n", "                       llabel = \"Degree = {:d}\".format(degree), show_test=0)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Generalization\n", "---\n", "##The training data is split into 2 subsets - training set, and the validation set\n", "##Models of increasing complexity are fitted to the training set\n", "##The Mean Squared Error is measured for the model predictions on the training set and the (unseen) validation set\n", "##Note that as the complexity or size of the model increases the MSE on the training set continues to fall\n", "##BUT the MSE on the validation set reaches a minimum and then rises again. The model is failing to generalize on unseen data as it over-fits"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Generalization, Bias, Variance as a function of the degree of the polynomial\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": ["np.random.seed(9)\n", "m = 100\n", "#x_plot = np.linspace(0, 1, m)\n", "X = np.random.uniform(0, 1, size=m)[:, np.newaxis] \n", "y = f(X) + np.random.normal(scale=0.3, size=m)[:, np.newaxis]\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n", "\n", "max_degree = 10\n", "training_error = np.zeros(max_degree)\n", "testing_error = np.zeros(max_degree)\n", "\n", "for degree in xrange(max_degree):\n", "    clf = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n", "    clf.fit(X_train, y_train)\n", "    training_error[degree] = mean_squared_error(y_train, clf.predict(X_train))\n", "    testing_error[degree] = mean_squared_error(y_test, clf.predict(X_test))\n", "\n", "\n", "fig = plt.figure(figsize=(5,5))\n", "ax = plt.subplot(111)\n", "ax.plot(np.arange(10), training_error, 'g', label='Training Error')\n", "ax.plot(np.arange(10), testing_error, 'r', label='Validation Error')\n", "ax.set_yscale('Log')\n", "ax.set_xlabel('Degree')\n", "ax.set_ylabel('Log(MSE)')\n", "ax.legend(loc='lower left')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Over-fitting\n", "---\n", "\n", "##Look what happens to the magnitudes of the model parameters\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["fig = plt.figure(figsize=(20,40))\n", "ax1 = plt.subplot(421)\n", "ax2 = plt.subplot(422)\n", "ax3 = plt.subplot(423)\n", "ax4 = plt.subplot(424)\n", "ax5 = plt.subplot(425)\n", "ax6 = plt.subplot(426)\n", "ax7 = plt.subplot(427)\n", "ax8 = plt.subplot(428)\n", "axes = [[ax1, ax2], [ax3, ax4], [ax5, ax6], [ax7, ax8]]\n", "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n", "\n", "for i, degree in enumerate([2, 4, 8, 10]):\n", "    left_axes = axes[i][0]\n", "    right_axes = axes[i][1]\n", "    clf = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n", "    clf.fit(X_train, y_train)\n", "    plot_approximation(x_plot, X_train, y_train, clf, left_axes, llabel = \"Degree = {:d}\".format(degree))\n", "    plot_model_parameters(clf, right_axes, llabel=None)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Addressing the over-fitting problem\n", "---\n", "## * Plot the hypothesis\n", "###     - usually of limited value because we usually have a lot of features\n", "\n", "---\n", "\n", "## * Reduce the number of features\n", "###     - select which features to keep, and which to throw out\n", "###     - but maybe all of the features are useful. Throwing away features may not be a good idea\n", "\n", "---\n", "\n", "## * Regularization\n", "###     - Keep all/most of the features, but reduce the magnitudes of the parameters of the model ($\\theta$s)\n", "###     - Works well when we have a lot of features, all of which might contribute a little towards predicting $y$\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Regularization\n", "---\n", "\n", "##What would happen if we were to add to our sum of squares cost function the following terms?\n", "## $$J(\\theta)=\\frac{1.0}{2m}\\left[\\sum_{i=1}^{m}(y_{i}-\\hat{y}_{i})^2 + 1000 * \\theta_{3} + 1000 * \\theta_{4}\\right]$$\n", "##In addition to minimizing the sum of squares term, $\\theta_{3}$ and $\\theta_{4}$ would both be driven towards $0$\n", "\n", "---\n", "\n", "##More generally: $$J(\\theta)=\\frac{1.0}{2m}\\left[\\sum_{i=1}^{m}(y_{i}-\\hat{y}_{i})^2 + \\lambda\\sum_{j}^{N}\\theta_{j}\\right]$$\n", "\n", "---\n", "\n", "##By adding an extra term that penalizes large model parameters we can produce models with small parameter values, which prevents over-fitting\n", "\n", "---\n", "#Intuition\n", "---\n", "##Small parameter values:\n", "###Produce a 'less complex' hypothesis function - 'smoother'\n", "###Produce a linear model that is less prone to over-fitting\n", "###One approach is to shrink all the parameters - all the $\\theta$s, using a single regularization parameter $\\lambda$\n", "###$$J(\\theta)=\\frac{1.0}{2m}\\left[\\sum_{i=1}^{m}(y_{i}-\\hat{y}_{i})^2 + \\lambda\\sum_{j}^{N}\\theta_{j}\\right]$$\n", "###If $\\lambda$ is set too high you will under-fit (again)!! - straight line\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Types of regularization\n", "---\n", "\n", "##There are different types of regularization terms available, and these are built into different regression algorithms:\n", "\n", "## 1. Ridge (Tikhonov regularization) = L2-norm = Euclidean norm of the sum of the parameters, $\\theta$, of the model $= \\lambda||\\theta||^{2}$\n", "###As the penalty is increased ALL parameters shrink, while still remaining non-zero\n", "\n", "---\n", "\n", "## 2. Lasso (Least Absolute Shrinkage and Selection Operator) = L1-norm $= \\lambda||\\theta||$\n", "###As the penalty is increased MORE of the parameters will shrink to zero\n", "###This can discard features\n", "\n", "---\n", "\n", "## 3. Elastic Net. A linear combination of Ridge and Lasso\n", "\n", "---"], "cell_type": "markdown", "metadata": {}}, {"source": ["#Sklearn - the regularization hyper-parameter is $\\alpha$, alpha"], "cell_type": "markdown", "metadata": {}}, {"source": ["#Ridge Regression"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": ["#same routine as above except we use Ridge rather than simple least squares regression\n", "fig = plt.figure(figsize=(20,40))\n", "ax1 = plt.subplot(421)\n", "ax2 = plt.subplot(422)\n", "ax3 = plt.subplot(423)\n", "ax4 = plt.subplot(424)\n", "ax5 = plt.subplot(425)\n", "ax6 = plt.subplot(426)\n", "ax7 = plt.subplot(427)\n", "ax8 = plt.subplot(428)\n", "axes = [[ax1, ax2], [ax3, ax4], [ax5, ax6], [ax7, ax8]]\n", "\n", "#Let's see the effect of regularizing with the following parameters\n", "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n", "\n", "#set the polynomial to an arbitrarily high degree\n", "degree=9\n", "\n", "for axis, aalpha in enumerate(alphas):\n", "    left_axes = axes[axis][0]\n", "    right_axes = axes[axis][1]\n", "    \n", "    #Use Ridge\n", "    clf = make_pipeline(PolynomialFeatures(degree), Ridge(alpha = aalpha))\n", "    clf.fit(X_train, y_train)\n", "    plot_approximation(x_plot, X_train, y_train, clf, \\\n", "                       left_axes, llabel = \"Alpha = {:0.8f}\".format(aalpha))\n", "    plot_model_parameters(clf, right_axes, llabel=None)\n", "    #print clf.steps[-1][1].coef_"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Lasso Regression\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": ["#Same routine as above, except we will use Lasso regression\n", "fig = plt.figure(figsize=(20,40))\n", "ax1 = plt.subplot(421)\n", "ax2 = plt.subplot(422)\n", "ax3 = plt.subplot(423)\n", "ax4 = plt.subplot(424)\n", "ax5 = plt.subplot(425)\n", "ax6 = plt.subplot(426)\n", "ax7 = plt.subplot(427)\n", "ax8 = plt.subplot(428)\n", "axes = [[ax1, ax2], [ax3, ax4], [ax5, ax6], [ax7, ax8]]\n", "\n", "#Here is the list of regularization hyper-parameters we will try\n", "alphas = [1e-10, 1e-8, 1e-5, 1e-2]\n", "#Set an arbitrarily high degree polynomial\n", "degree=9\n", "\n", "for axis, aalpha in enumerate(alphas):\n", "    left_axes = axes[axis][0]\n", "    right_axes = axes[axis][1]\n", "    \n", "    #Use Lasso\n", "    clf = make_pipeline(PolynomialFeatures(degree), Lasso(alpha = aalpha))\n", "    clf.fit(X_train, y_train)\n", "    plot_approximation(x_plot, X_train, y_train, clf, \\\n", "                       left_axes, llabel = \"Alpha = {:0.8f}\".format(aalpha))\n", "    plot_model_parameters(clf, right_axes, llabel=None, ylim_min=0.1)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "#Regularized Linear Regression (gradient descent and linear equations)\n", "---\n", "## * In gradient descent the addition of the regularization term shrinks the parameters on each iteration\n", "## * In the normal equations a similar thing happens in the math analytically\n", " \n", "--- \n", " ##Sklearn Ridge/Lasso/ElasticNet Regression chooses internally how it will fit the linear model - iterative or analytic. Mostly they appear to use iterative gradient descent-like algorithms to solve for the linear model parameters\n", " ---\n", " ##Sklearn Lasso is identical to simple least squares LinearRegression() if $\\alpha = 0$.\n", " ---"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "#Generalization, Bias, Variance as a function of the Regularization Parameter\n", "---"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": ["#Similar to the Generalization, Bias, Variance as a function of degree of polynomial plot above\n", "np.random.seed(9)\n", "m = 100\n", "X = np.random.uniform(0, 1, size=m)[:, np.newaxis] \n", "y = f(X) + np.random.normal(scale=0.3, size=m)[:, np.newaxis]\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n", "\n", "#This time set up a very large complex polynomial\n", "max_degree = 25\n", "\n", "#We will investigate the following regularization hyper-parameters\n", "alphas = [0.0, 1e-8, 1e-6, 1e-4, 1e-2, 1, 100]\n", "\n", "training_error = []\n", "testing_error = []\n", "\n", "for a in alphas:\n", "    #Use Ridge as an example\n", "    clf = make_pipeline(PolynomialFeatures(max_degree), Ridge(alpha=a))\n", "    \n", "    #Fit the model\n", "    clf.fit(X_train, y_train)\n", "    \n", "    #Accumulate the errors\n", "    training_error.append(mean_squared_error(y_train, clf.predict(X_train)))\n", "    testing_error.append(mean_squared_error(y_test, clf.predict(X_test)))\n", "\n", "for i in xrange(len(training_error)):\n", "    print \"{:18.5f}\".format(training_error[i]),\n", "    \n", "print \"\\n\"\n", "\n", "for i in xrange(len(testing_error)):\n", "    print \"{:18.5f}\".format(testing_error[i]),\n", "\n", "print \"\\n\"\n", "\n", "#Plot the training and test errors against the value of the regularization hyper-pararmeter    \n", "fig = plt.figure(figsize=(5,5))\n", "ax = plt.subplot(111)\n", "ax.plot(np.array(alphas), np.array(training_error), color = 'green', marker = 'o', label='Training Error')\n", "ax.plot(np.array(alphas), np.array(testing_error), color = 'red', marker = 'o', label='Validation')\n", "ax.set_yscale('Log')\n", "ax.set_ylim(0.01, 100000)\n", "ax.set_xscale('Log')\n", "ax.set_xlabel('Lambda (Alpha)')\n", "ax.set_ylabel('Log(MSE)')\n", "ax.legend(loc='upper right')"], "outputs": [], "metadata": {"collapsed": false}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.9", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}