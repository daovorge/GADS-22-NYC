#Lesson 13 Naive Bayes
###July 15th 2015

###Objectives for this class:
 * To define the difference between a 'frequentist' and a 'Bayesian'
 * To illustrate and define Joint, Marginal and Conditional probabilities/distributions
 * To list and define the 2 rules of probability
 * To deduce and define Bayes' Theorem
 * To define Independence
 * To calculate, using the rules of probability and Bayes Theorem, the answers to the posed probability questions
 * To introduce and discuss Bayes Classifiers
 * To define a Naive Bayes' Classifiers, and precisely idenitfy the reason for the 'naive' tag
 * to define what a count vectorizer is and what an n-gram is
 
###Class Agenda
 - Class Open
  * Check in 
  * Mid-term assessment questionaire - 20 mins
  * Review objectives
 - Bayes - Core concepts - Slides - Mark
 - Bernoulli Distribution - short iPython notebook - Mark & Class
 - Multinomial Distribution - short iPython notebook - Mark & Class
 - Bayes and Basic Probability - iPython notebook - Mark & Class
 - LAB: Predicting movie critique text as 'rotten' or 'fresh' - Coding exercise - Class
 - Class Close
  * Check in
  * Class to pause around 9.15pm for the Exit ticket
  * Wrap up
 
 
### Term Project
  Answer the following questions:
  1. What is your topic? 
  2. Can you phrase your topic in the form of a question that you hope to answer?
  3. What do you plan to use as your source of data? 
  4. Do you have a sense for how large your dataset is? 
  5. Any other characteristics you know of?
  6. What tools or topics do you hope to learn and demonstrate by the end, or in other words, what are your learning objectives?


###Additional Resources
* [Naive Bayes Math](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)
* [Naive Bayes Test Classification](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)
* Beaware of the linkedin Data Science group - plenty of blogs, discussion and job postings
* [Data Science Central](http://www.datasciencecentral.com/) For discussions, conferences, jobs, etc
* Meetups
* [Andrew Ng's coursera course](https://www.coursera.org/learn/machine-learning/home/info), which is a great machine learning course. The notation I have used matches his. His course used Octave (like matlab), so unless you are dead keen to learn this, you can ignore the programming exercises. The course is a little mathematical at times, but overall he presents great videos about gradient descent, and linear models.


