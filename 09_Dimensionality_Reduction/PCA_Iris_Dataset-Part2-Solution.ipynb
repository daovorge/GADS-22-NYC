{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import operator\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#The Iris Dataset Part 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#The Iris Dataset is a builtin dataset, you need from sklearn import datasets - see above\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Construct a Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {'class':iris.target, 'sepal_length':iris.data[:,0], 'sepal_width':iris.data[:,1], \\\n",
    "      'petal_length':iris.data[:,2], 'petal_width':iris.data[:,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'petal_length', u'petal_width', u'sepal_length', u'sepal_width'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#We will need a list of features only\n",
    "features = df.columns[1:]\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.7</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.0</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.0</td>\n",
       "      <td> 6.5</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.4</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 6.2</td>\n",
       "      <td> 3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td> 2</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 5.9</td>\n",
       "      <td> 3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  petal_length  petal_width  sepal_length  sepal_width\n",
       "145      2           5.2          2.3           6.7          3.0\n",
       "146      2           5.0          1.9           6.3          2.5\n",
       "147      2           5.2          2.0           6.5          3.0\n",
       "148      2           5.4          2.3           6.2          3.4\n",
       "149      2           5.1          1.8           5.9          3.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Put the scaled data into a new dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True).fit(df[features])\n",
    "df_scaled = pd.DataFrame(scaler.transform(df[features]), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scaled['class'] = df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-0.900681</td>\n",
       "      <td> 1.032057</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.385353</td>\n",
       "      <td> 0.337848</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.506521</td>\n",
       "      <td> 0.106445</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>-1.021849</td>\n",
       "      <td> 1.263460</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width  sepal_length  sepal_width  class\n",
       "0     -1.341272    -1.312977     -0.900681     1.032057      0\n",
       "1     -1.341272    -1.312977     -1.143017    -0.124958      0\n",
       "2     -1.398138    -1.312977     -1.385353     0.337848      0\n",
       "3     -1.284407    -1.312977     -1.506521     0.106445      0\n",
       "4     -1.341272    -1.312977     -1.021849     1.263460      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Define a custom confusion matrix function \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix(predictions, y, names):\n",
    "    '''This function uses the pd.crosstab function to create a confusion matrix:\n",
    "    predictions are the predictions from the predictive mode\n",
    "    y are the known class labels\n",
    "    names are the names of the features used in the model'''\n",
    "    \n",
    "    cf = pd.crosstab(y, predictions)\n",
    "    cf.columns = names\n",
    "    cf.index = names\n",
    "    cf.columns.name = 'Prediction'\n",
    "    cf.index.name = 'Actual'\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Full Training with Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_optimal_features(dfs, target, all_features, \n",
    "                          max_input_dimension = 5,\n",
    "                          degree_list=[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10],\n",
    "                          reg_list = [1.0]):\n",
    "    \n",
    "    #initialize some results dictionarys\n",
    "    results = {}\n",
    "    results_vars  = {}\n",
    "    #count will be used to index the dictionarys\n",
    "    count = 0\n",
    "    \n",
    "    #Convert the dataframe data to user-friendly arrays\n",
    "    X = dfs[all_features].values\n",
    "    y = dfs[target].values\n",
    "   \n",
    "    #for classification I use the Stratified Shuffle Split for cross validation purposes\n",
    "    sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=32) \n",
    "    \n",
    "    #split the data first and the training set is what is used for all model parameters!!\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "        \n",
    "        #look for a good degree for the polynomial features\n",
    "        for degree in degree_list:\n",
    "    \n",
    "            pf_model = PolynomialFeatures(degree).fit(X_train)\n",
    "            pf = pf_model.transform(X_train)\n",
    "        \n",
    "            #if the polynomial features are not enough to satisfy the max input dimension, as specified by the user\n",
    "            #then reduce the max input dimension\n",
    "            if max_input_dimension > pf.shape[1]:\n",
    "                max_input_dimension = pf.shape[1]\n",
    "            \n",
    "            #now use PCA to try models of varying dimensions\n",
    "            for input_dimension in xrange(1, max_input_dimension+1):\n",
    "        \n",
    "                myPCA = PCA(n_components = input_dimension).fit(pf)\n",
    "            \n",
    "                #The final transformed dataset\n",
    "                X_transform = myPCA.transform(pf)\n",
    "        \n",
    "                #Regularize the model\n",
    "                for reg_C in reg_list:\n",
    "                \n",
    "                    #Finally build and fit the logistic regression model\n",
    "                    clfLR = LogisticRegression(C=reg_C)\n",
    "                    clfLR.fit(X_transform, y_train)\n",
    "    \n",
    "                    #Now test the performance on the validation set\n",
    "                    #But first we have to prepare the test set data\n",
    "                    #So get the non-linear features using the polynomial model\n",
    "                    pf_test = pf_model.transform(X_test)\n",
    "                    \n",
    "                    #Now get the PCA features using the PCA model\n",
    "                    X_test_transform = myPCA.transform(pf_test)\n",
    "                    \n",
    "                    #And finally get a predication from the logistic regression model\n",
    "                    my_score = clfLR.score(X_test_transform, y_test)\n",
    "                    \n",
    "                    #We accumulate all results as we might like to look at more than just the 'best'\n",
    "                    results[count] = my_score\n",
    "                    results_vars[count] = [input_dimension, degree, reg_C, \\\n",
    "                                           clfLR, X_transform, y_train, X_test_transform, y_test, \n",
    "                                           pf_model, myPCA]\n",
    "                    count += 1\n",
    "                    \n",
    "    return (results, results_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up the call\n",
    "scores, input_vars = find_optimal_features(df_scaled, 'class', features, \n",
    "                                           max_input_dimension = 8, \n",
    "                                           degree_list = [2, 3, 4, 5, 6], \n",
    "                                           reg_list = np.logspace(-4, 6, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-04,   1.29154967e-03,   1.66810054e-02,\n",
       "         2.15443469e-01,   2.78255940e+00,   3.59381366e+01,\n",
       "         4.64158883e+02,   5.99484250e+03,   7.74263683e+04,\n",
       "         1.00000000e+06])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logspace test area - to see what the regularizer looks like\n",
    "np.logspace(-4, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the top ten models, based upon the scores\n",
    "top_ten = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 144\n",
      "score = 0.98667 In Dim = 7 Deg = 3 C =  2.7826\n",
      "\n",
      "\n",
      "model number 53\n",
      "score = 0.97333 In Dim = 6 Deg = 2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 63\n",
      "score = 0.97333 In Dim = 7 Deg = 2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 73\n",
      "score = 0.97333 In Dim = 8 Deg = 2 C =  0.2154\n",
      "\n",
      "\n",
      "model number 125\n",
      "score = 0.97333 In Dim = 5 Deg = 3 C = 35.9381\n",
      "\n",
      "\n",
      "model number 126\n",
      "score = 0.97333 In Dim = 5 Deg = 3 C = 464.1589\n",
      "\n",
      "\n",
      "model number 143\n",
      "score = 0.97333 In Dim = 7 Deg = 3 C =  0.2154\n",
      "\n",
      "\n",
      "model number 145\n",
      "score = 0.97333 In Dim = 7 Deg = 3 C = 35.9381\n",
      "\n",
      "\n",
      "model number 153\n",
      "score = 0.97333 In Dim = 8 Deg = 3 C =  0.2154\n",
      "\n",
      "\n",
      "model number 215\n",
      "score = 0.97333 In Dim = 6 Deg = 4 C = 35.9381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's examine the top ten models, and check out the input dimension, the polynomial degree and the regularizer\n",
    "for i in xrange(len(top_ten)):\n",
    "    print \"model number {:d}\".format(top_ten[i][0])\n",
    "    print \"score = {:5.5}\".format(top_ten[i][1]), \\\n",
    "    \"In Dim = {:d}\".format(input_vars[top_ten[i][0]][0]), \\\n",
    "    \"Deg = {:d}\".format(input_vars[top_ten[i][0]][1]), \\\n",
    "    \"C = {:7.4f}\".format(input_vars[top_ten[i][0]][2])\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model score = 0.98667\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           1         24\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Model score = 0.97333\n",
      "Prediction  setosa  versicolor  virginica\n",
      "Actual                                   \n",
      "setosa          25           0          0\n",
      "versicolor       0          25          0\n",
      "virginica        0           2         23\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#But we are not done yet. We need to look at the confusion matrices for all the top models\n",
    "\n",
    "for i in xrange(5):\n",
    "    tn = top_ten[i][0]\n",
    "    model = input_vars[tn][3]\n",
    "    X_test = input_vars[tn][6]\n",
    "    y_test = input_vars[tn][7]\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    print \"----------------------------------------\"\n",
    "    print \"Model score = {:5.5f}\".format(model.score(X_test, y_test))\n",
    "    cm = my_confusion_matrix(y_hat, y_test, iris.target_names)\n",
    "    print cm\n",
    "    print \"----------------------------------------\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is predicted to belong to class 1 versicolor\n",
      "...with the following probabilities 0.374272850942 0.62572714802 1.03830398805e-09\n"
     ]
    }
   ],
   "source": [
    "#Using the best model let's predict something new\n",
    "tn = top_ten[0][0]\n",
    "new_input = [1.5, 0.45, 8.2, 3.7]\n",
    "\n",
    "#input scaling first\n",
    "new_input_scaled = scaler.transform(new_input)\n",
    "\n",
    "#now the polynomial features\n",
    "pf = input_vars[tn][8]\n",
    "p_input = pf.transform(new_input_scaled)\n",
    "\n",
    "#and now the PCA\n",
    "myPCA = input_vars[tn][9]\n",
    "new_X_transform = myPCA.transform(p_input)\n",
    "\n",
    "#and now predict using the LR model\n",
    "clfLR = input_vars[tn][3]\n",
    "pred = clfLR.predict(new_X_transform)\n",
    "print \"This data is predicted to belong to class {:d}\".format(pred[0]), \\\n",
    "    iris.target_names[pred[0]]\n",
    "prob = clfLR.predict_proba(new_X_transform)\n",
    "print \"...with the following probabilities\",\n",
    "for i in xrange(3):\n",
    "    print prob.ravel()[i],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
