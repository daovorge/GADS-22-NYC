{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Naive Bayes Classification - Predicting Movie Quotes\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Count Vectorization\n",
    "###Obtaining word frequencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(text):\n",
    "    print \"{:d} = {:s}\".format(i, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngr = (1, 1)\n",
    "myCVa = CountVectorizer(ngram_range = ngr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####As per usual you fit the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCVa.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transf_text = myCVa.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Then use the transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, fn in enumerate(myCVa.get_feature_names()):\n",
    "    print \"{:2d} = {:s}\".format(i, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The transform method returns a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print transf_text\n",
    "#(A, B)  C\n",
    "#A refers to the text number. There were 3 sentences so you would expect 0, 1, 2\n",
    "#The 0 text is \"Math is great\"\n",
    "#B refers to the feature. There are 5 features so you would expect 0, 1, 2, 3, 4\n",
    "#The 1 feature is 'great', 2 is 'is', 3 is 'math'\n",
    "#C is the frequency of the word in the text\n",
    "#There are 2 instances of 'exciting' in the 3rd sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Which you can visualize as a dense matrix\n",
    "#####There are 3 sentences and 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print transf_text.todense()\n",
    "print transf_text.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##What is an ngram?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Set the begining n-gram length to 1\n",
    "#####Set the end n-gram length to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngr = (1, 2)\n",
    "myCVb = CountVectorizer(ngram_range = ngr)\n",
    "myCVb.fit(text)\n",
    "transformed_text = myCVb.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Now look at the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, fn in enumerate(myCVb.get_feature_names()):\n",
    "    print \"{:2d} = {:s}\".format(i, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(text):\n",
    "    print \"{:d} = {:s}\".format(i, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print transformed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print transformed_text.shape\n",
    "print type(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_array = transformed_text.toarray()\n",
    "print dense_array.shape\n",
    "print dense_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Understanding the sparse array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(3):\n",
    "    for j in xrange(11):\n",
    "        if dense_array[i][j] == 1:\n",
    "            print i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Questions:\n",
    "===\n",
    "1. What does 1 10 represent?\n",
    " \n",
    "2. What does 0 8 represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The Movie Database\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "critics_data = pd.read_csv(\"/Users/mrgholt/GADS-22-NYC/Datasets/rt_critics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in critics_data.columns:\n",
    "    print type(i), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "critics_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO Determine what values 'fresh' can take in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Create a new data frame that contains only records where 'fresh' has values 'fresh' and 'rotten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Create a 'y' variable which is the factorization of the 'fresh' column, so 'fresh' gets converted to 0 (or 1)\n",
    "#and 'rotten' gets converted to 1 (or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: How many of each class ('fresh' and 'rotten') do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Now using CountVectorizer fit and transform the quotes, start using a n-gram size of (1-1)\n",
    "#hint: you can fit and transform in one hit using \"fit_transform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here is an example of quote that is associated with the 'fresh' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_quote = fdc.quote[fdc.fresh == 'fresh'][50:51].values[0]\n",
    "print fdc.fresh[fdc.fresh == 'fresh'][50:51].values[0]\n",
    "print test_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ...and here is an example of a quote that is associated with the 'rotten' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_quote = fdc.quote[fdc.fresh == 'rotten'][50:51].values[0]\n",
    "fdc.fresh[fdc.fresh == 'rotten'][50:51].values[0]\n",
    "print test_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Using train test split fit a mulitnomialNB model and a BernoulliNB model to the training data\n",
    "#TODO: What accuracy to do you get on the test set for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: now test some of your own review language in a very short sentence and see if your review is predicted as\n",
    "#fresh or rotten\n",
    "#If you want to use the helper function below\n",
    "#Remember: use the count vectorizer transform method to convert your quote into a sparse array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def report_results(clf, my_sparse, my_review):\n",
    "    #get the probabilities from the model\n",
    "    res_prob = clf.predict_proba(my_sparse).ravel()\n",
    "    \n",
    "    #set up the correct output string\n",
    "    if clf.predict(my_sparse) == 0:\n",
    "        result = \"fresh\"\n",
    "        \n",
    "    else:\n",
    "        result = \"rotten\"\n",
    "    \n",
    "    #print the result making sure the correct probability is used\n",
    "    print \"Your review:\\n '\", my_review, \"'\", \"\\n\\nhas a {:5.2f}% chance of being classified in the '{:s}' class\".\\\n",
    "    format(res_prob[clf.predict(my_sparse)][0] * 100.0, result)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Here are some test quotes to start you off testing your models\n",
    "#####Try individual words, try phrases - explore the space a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_quotes = [\"This was an awesome movie\", \\\n",
    "              \"This movie was so self indulgant that it really couldn't get over itself\", \\\n",
    "              \"So ingenious in concept\", \\\n",
    "              \"A gloomy special-effects extravaganza filled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: print out a pd crosstab for the predictors using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Questions:\n",
    "===\n",
    "1. n-gram settings gives the best results - 1, 2, or 3?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
