{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = fetch_covtype(random_state=11, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Covertype dataset\n",
    "\n",
    "####580,000+ examples of data obtained from 30 x 30 meter patches of US Forest\n",
    "####The aim of collecting the data was to predict the dominant species of tree for each patch (covertype)\n",
    "####7 species of tree have been labelled to create a 7 class prediction problem\n",
    "####From each patch a total of 54 features have been extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest covertype dataset.\n",
      "\n",
      "A classic dataset for classification benchmarks, featuring categorical and\n",
      "real-valued features.\n",
      "\n",
      "The dataset page is available from UCI Machine Learning Repository\n",
      "\n",
      "    http://archive.ics.uci.edu/ml/datasets/Covertype\n",
      "\n",
      "Courtesy of Jock A. Blackard and Colorado State University.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covertypes = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix(y_test, y_hat, names):\n",
    "    '''This function uses the pd.crosstab function to create a confusion matrix:\n",
    "    predictions are the predictions from the predictive mode\n",
    "    y are the known class labels\n",
    "    names are the names of the features used in the model'''\n",
    "    \n",
    "    cf = pd.crosstab(y_test, y_hat)\n",
    "    cf.columns = names\n",
    "    cf.index = names\n",
    "    cf.columns.name = 'Prediction'\n",
    "    cf.index.name = 'Actual'\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 54)\n"
     ]
    }
   ],
   "source": [
    "size = data.data\n",
    "print size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_shuffle(X, y, M):\n",
    "    '''This function shuffles the order of the data set and keeps the labels in synch'''\n",
    "    Xa = X.copy()\n",
    "    ya = y.copy()\n",
    "    \n",
    "    for i in xrange(100000):\n",
    "        n1 = randint(0, M-1)\n",
    "        n2 = randint(0, M-1)\n",
    "        \n",
    "        tempx = Xa[n1, :].copy()\n",
    "        tempy = ya[n1].copy()\n",
    "\n",
    "        Xa[n1, :] = Xa[n2, :].copy()\n",
    "        ya[n1] = ya[n2].copy()\n",
    "\n",
    "        Xa[n2, :] = tempx\n",
    "        ya[n2] = tempy\n",
    "    \n",
    "    return (Xa, ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes here (3500, 54) (3500, 1)\n",
      "class  1   500\n",
      "class  2   500\n",
      "class  3   500\n",
      "class  4   500\n",
      "class  5   500\n",
      "class  6   500\n",
      "class  7   500\n",
      "\n",
      "\n",
      "\n",
      "shapes here (3500, 54) (3500, 1)\n",
      "class  1   500\n",
      "class  2   500\n",
      "class  3   500\n",
      "class  4   500\n",
      "class  5   500\n",
      "class  6   500\n",
      "class  7   500\n"
     ]
    }
   ],
   "source": [
    "M = 14000\n",
    "N = 54\n",
    "n_classes = 7\n",
    "#specify number per class\n",
    "my_size = 500\n",
    "\n",
    "X1 = np.zeros((n_classes * my_size, N))\n",
    "y1 = np.zeros((n_classes * my_size, 1))\n",
    "features = range(N)\n",
    "\n",
    "df1 = pd.DataFrame(data.data)\n",
    "df1[\"y\"] = data.target\n",
    "\n",
    "for i in xrange(1, n_classes+1):\n",
    "    j = i-1\n",
    "    X1[j*my_size:i*my_size, :] = df1[df1.y == i][features][0:my_size].values\n",
    "    y1[j*my_size:i*my_size, :] = i\n",
    "\n",
    "print \"shapes here\", X1.shape, y1.shape\n",
    "\n",
    "#This is just a little sanity check that X1, and X2 have the correct number of samples in each class\n",
    "y1 = y1.ravel()\n",
    "td = {}\n",
    "for i in xrange(1, n_classes+1):\n",
    "    td[i] = 0\n",
    "for i in xrange(len(y1)):\n",
    "    td[y1[i]] += 1\n",
    "for i in xrange(1, n_classes+1):\n",
    "    print \"class \", i, \" \", td[i]\n",
    "print \"\\n\\n\"\n",
    "y1 = y1.reshape(y1.shape[0], 1)\n",
    "print \"shapes here\", X1.shape, y1.shape\n",
    "\n",
    "#Now shuffle the pack\n",
    "X2, y2 = my_shuffle(X1, y1, n_classes * my_size)\n",
    "\n",
    "y2 = y2.ravel()\n",
    "td = {}\n",
    "for i in xrange(1, n_classes+1):\n",
    "    td[i] = 0\n",
    "for i in xrange(len(y2)):\n",
    "    td[y2[i]] += 1\n",
    "for i in xrange(1, n_classes+1):\n",
    "    print \"class \", i, \" \", td[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Xs = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True).fit_transform(X2)\n",
    "Xs = X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Example Bagging\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Sklearn Bagging Classifier encompasses a number of the averaging methods\n",
    "###It has a comprehensive argument list:\n",
    "\n",
    "- base_estimator is the classifier you are going to use with bagging\n",
    "- n_estimators is the number of base estimators to use\n",
    "- max_samples is the number of samples to draw from the complete training set, each random sample is used to train one of the base estimators\n",
    "- max_features is the number of features to use from the complete training set\n",
    "- bootstrap, if True, indicates to the algorithm to draw with replacement (remember you replace after each sample draw)\n",
    "- bootstrap_features, if True, indicates to the algorithm to draw the features with replacement\n",
    "- n_jobs allows for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#####Try the base classifier first\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mySSS = StratifiedShuffleSplit(y2, 1, test_size=0.5, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682857142857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.56      0.48      0.52       250\n",
      "        2.0       0.52      0.43      0.47       250\n",
      "        3.0       0.66      0.57      0.61       250\n",
      "        4.0       0.78      0.88      0.83       250\n",
      "        5.0       0.76      0.88      0.81       250\n",
      "        6.0       0.65      0.69      0.67       250\n",
      "        7.0       0.78      0.85      0.81       250\n",
      "\n",
      "avg / total       0.67      0.68      0.67      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfKNN = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfKNN.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = clfKNN.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#####Now wrap the Bagging Classifier around the base classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702857142857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.60      0.50      0.54       250\n",
      "        2.0       0.55      0.42      0.48       250\n",
      "        3.0       0.73      0.59      0.65       250\n",
      "        4.0       0.79      0.92      0.85       250\n",
      "        5.0       0.72      0.90      0.80       250\n",
      "        6.0       0.67      0.70      0.68       250\n",
      "        7.0       0.79      0.89      0.84       250\n",
      "\n",
      "avg / total       0.69      0.70      0.69      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfBag = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors = 1), n_estimators = 100, max_samples = 0.7,\\\n",
    "                          max_features = 0.7, random_state = 6, n_jobs = -1)\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfBag.fit(X_train, y_train)\n",
    "\n",
    "    y_hat = clfBag.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Introducing cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701714285714\n",
      "0.00514285714286\n"
     ]
    }
   ],
   "source": [
    "clfBagB = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors = 1), n_estimators = 100, max_samples = 0.7,\\\n",
    "                          max_features = 0.7, random_state = 6, n_jobs = 1)\n",
    "\n",
    "#NB: cross_val_score does the cross validation for you\n",
    "#VIP: If you do use cross_val_score with a separate train test split, then you can easily get train, xvalidation and test sets - see grid search notebook\n",
    "#cv = number of folds of cross validation to do\n",
    "#n_jobs allows for parallezied execution, which sometimes gives problems inside interpreters\n",
    "\n",
    "scores = cross_val_score(clfBagB, Xs, y2, cv=2, scoring = 'accuracy', n_jobs = -1)\n",
    "print np.mean(scores)\n",
    "print np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Patches Example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Sklearn Ensemble Tree or Patches Classifiers have a number of important arguments\n",
    "####Random Forests\n",
    "####Extra Trees\n",
    "\n",
    "- max_features is the number of features to consider when looking for the best split, the default is all of the features \n",
    "- max_depth is the maximum depth of the tree. The default is to have a tree such that each leaf node is 'pure'. This parameter is affected by 2 others: 1. min_samples_split, and, 2. max_leaf_nodes\n",
    "- min_samples_split is the minimum number of samples required to split an internal node, the default being 2\n",
    "- min_samples_leaf is the number of samples in newly created leaves\n",
    "- max_leaf_nodes grows a tree until the leaves have this number. Setting this means max_depth will be ignored\n",
    "- bootstrap will allow the model to use bootstrapped samples\n",
    "- n_estimators is the number of trees in the forest\n",
    "- criterion is again is either 'entropy' or 'gini'\n",
    "- n_jobs will parallelize and speed things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768571428571\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.67      0.66      0.67       250\n",
      "        2.0       0.65      0.49      0.56       250\n",
      "        3.0       0.75      0.71      0.73       250\n",
      "        4.0       0.86      0.96      0.91       250\n",
      "        5.0       0.79      0.85      0.82       250\n",
      "        6.0       0.73      0.75      0.74       250\n",
      "        7.0       0.87      0.94      0.91       250\n",
      "\n",
      "avg / total       0.76      0.77      0.76      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfRF = RandomForestClassifier(n_estimators = 150, n_jobs = -1, random_state = 11)\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfRF.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = clfRF.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763428571429\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.67      0.65      0.66       250\n",
      "        2.0       0.63      0.53      0.58       250\n",
      "        3.0       0.74      0.67      0.71       250\n",
      "        4.0       0.87      0.95      0.91       250\n",
      "        5.0       0.82      0.82      0.82       250\n",
      "        6.0       0.69      0.77      0.73       250\n",
      "        7.0       0.89      0.94      0.91       250\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfET = ExtraTreesClassifier(n_estimators = 150, n_jobs = -1,random_state = 11)\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfET.fit(X_train, y_train)\n",
    "    y_hat = clfET.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Boosting Example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ada Boost has the capability to boost another algorithm - the default being DecisionTreeClassifier\n",
    "\n",
    "####Other important parameters:\n",
    "\n",
    "- base_estimator specifies the model for boosting\n",
    "- n_estimators as above\n",
    "- learning_rate shrinks the contribution of each classifier, and so is a trade-off with n_estimators\n",
    "- no n_jobs, so cannot parallelize execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.60      0.01      0.02       250\n",
      "        2.0       0.00      0.00      0.00       250\n",
      "        3.0       0.31      0.94      0.47       250\n",
      "        4.0       0.00      0.00      0.00       250\n",
      "        5.0       0.38      0.91      0.54       250\n",
      "        6.0       0.00      0.00      0.00       250\n",
      "        7.0       0.62      0.98      0.76       250\n",
      "\n",
      "avg / total       0.27      0.41      0.26      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfAB = AdaBoostClassifier(n_estimators = 100, random_state = 11)\n",
    "\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfAB.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = clfAB.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684571428571\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.59      0.58      0.59       250\n",
      "        2.0       0.51      0.48      0.50       250\n",
      "        3.0       0.60      0.65      0.62       250\n",
      "        4.0       0.84      0.91      0.87       250\n",
      "        5.0       0.75      0.78      0.76       250\n",
      "        6.0       0.62      0.52      0.57       250\n",
      "        7.0       0.86      0.86      0.86       250\n",
      "\n",
      "avg / total       0.68      0.68      0.68      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfTree = DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfTree.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = clfTree.predict(X_test)\n",
    "    \n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GradientBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Gradient Boosting is a specific tree based model boosting algorithm\n",
    "####Other important parameters:\n",
    "- loss. If loss is set to 'exponential' then this algorithm is the same as AdaBoost. This algorithm differs, therefore, as a result of a different loss function\n",
    "- learning_rate - as for AdaBoost\n",
    "- n_estimators - as for AdaBoost with the same caveat\n",
    "- All of the Decision Tree parameters apply here too - max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes\n",
    "- no paralleziation option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733142857143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.66      0.65      0.65       250\n",
      "        2.0       0.64      0.48      0.54       250\n",
      "        3.0       0.67      0.68      0.67       250\n",
      "        4.0       0.87      0.94      0.91       250\n",
      "        5.0       0.74      0.81      0.78       250\n",
      "        6.0       0.67      0.65      0.66       250\n",
      "        7.0       0.83      0.92      0.88       250\n",
      "\n",
      "avg / total       0.73      0.73      0.73      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfGB = GradientBoostingClassifier(n_estimators = 100, random_state = 11)\n",
    "\n",
    "for train_index, test_index in mySSS:\n",
    "    X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "    y_train, y_test = y2[train_index], y2[test_index]\n",
    "    \n",
    "    clfGB.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = clfGB.predict(X_test)\n",
    "    print accuracy_score(y_test, y_hat)\n",
    "    print classification_report(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###Just checking the confusion matrices code is all OK\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162,  47,   0,   0,  10,   2,  29],\n",
       "       [ 64, 139,   6,   0,  32,   8,   1],\n",
       "       [  0,   2, 157,  24,   2,  65,   0],\n",
       "       [  0,   0,   6, 235,   0,   9,   0],\n",
       "       [  3,  18,   8,   0, 216,   5,   0],\n",
       "       [  1,   6,  54,  16,   4, 169,   0],\n",
       "       [ 24,   2,   0,   0,   0,   0, 224]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, clfGB.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>139</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    1    2    3    4    5    6    7\n",
       "Actual                                      \n",
       "1          162   47    0    0   10    2   29\n",
       "2           64  139    6    0   32    8    1\n",
       "3            0    2  157   24    2   65    0\n",
       "4            0    0    6  235    0    9    0\n",
       "5            3   18    8    0  216    5    0\n",
       "6            1    6   54   16    4  169    0\n",
       "7           24    2    0    0    0    0  224"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.crosstab(y_test, clfGB.predict(X_test), rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Spruce/Fir</th>\n",
       "      <th>Lodgepole Pine</th>\n",
       "      <th>Ponderosa Pine</th>\n",
       "      <th>Cottonwood/Willow</th>\n",
       "      <th>Aspen</th>\n",
       "      <th>Douglas-fir</th>\n",
       "      <th>Krummholz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spruce/Fir</th>\n",
       "      <td>162</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lodgepole Pine</th>\n",
       "      <td>64</td>\n",
       "      <td>139</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ponderosa Pine</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cottonwood/Willow</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspen</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Douglas-fir</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krummholz</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction         Spruce/Fir  Lodgepole Pine  Ponderosa Pine  \\\n",
       "Actual                                                          \n",
       "Spruce/Fir                162              47               0   \n",
       "Lodgepole Pine             64             139               6   \n",
       "Ponderosa Pine              0               2             157   \n",
       "Cottonwood/Willow           0               0               6   \n",
       "Aspen                       3              18               8   \n",
       "Douglas-fir                 1               6              54   \n",
       "Krummholz                  24               2               0   \n",
       "\n",
       "Prediction         Cottonwood/Willow  Aspen  Douglas-fir  Krummholz  \n",
       "Actual                                                               \n",
       "Spruce/Fir                         0     10            2         29  \n",
       "Lodgepole Pine                     0     32            8          1  \n",
       "Ponderosa Pine                    24      2           65          0  \n",
       "Cottonwood/Willow                235      0            9          0  \n",
       "Aspen                              0    216            5          0  \n",
       "Douglas-fir                       16      4          169          0  \n",
       "Krummholz                          0      0            0        224  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_confusion_matrix(y_test, clfGB.predict(X_test), covertypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
